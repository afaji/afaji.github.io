

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Research Statement - Alham Fikri Aji</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alham Fikri Aji">
<meta property="og:title" content="Research Statement">


  <link rel="canonical" href="https://afaji.github.io/tenure/research">
  <meta property="og:url" content="https://afaji.github.io/tenure/research">





  <meta name="twitter:site" content="@alhamfikri">
  <meta name="twitter:title" content="Research Statement">
  <meta name="twitter:description" content="NLP scientist - Assistant Professor at MBZUAI &amp; Monash Indonesia">
  <meta name="twitter:url" content="https://afaji.github.io/tenure/research">

  
    <meta name="twitter:card" content="summary">
    
  

  



  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alham Fikri Aji",
      "url" : "https://afaji.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://afaji.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alham Fikri Aji Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://afaji.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://afaji.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://afaji.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://afaji.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://afaji.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://afaji.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://afaji.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://afaji.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://afaji.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://afaji.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://afaji.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://afaji.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://afaji.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://afaji.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://afaji.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

    <style>
      @media print {
        .page-break {
            page-break-before: always;
        }
      }

      @media screen {
        /* Global font reset to ensure consistency */
        body, p, li, td, th {
          font-size: 0.9rem !important;
        }
      }
      @media print {
          body {
              font-size: 55%;
              -webkit-print-color-adjust: exact;
              print-color-adjust: exact; /* Standard property for non-webkit browsers */
          }
      }

      @media print {
        #main {
          padding-left: 5em !important;
          padding-right: 3em !important;
        }
      }

      #main {
        padding-left: 4em;
        padding-right: 2em;
      }

      ul li {
        margin-bottom: 0.5em;
        margin-top: 0.2em;
        list-style-type: circle;
      }
      
      .compact-ul ul li,
      #awards + ul li,
      #professional-activity + * + ul li {
        margin-bottom: 0.1em;
        margin-top: 0.0em;
        list-style-type: circle;
      }
      .page__content a[href^="#"] {
          color: #1E93AB;
          text-decoration: none;
          border-radius: 3px;
          padding: 0 3px;
          font-size: 0.95em;
          font-weight: 500;
      }
      .page__content a[href^="#"]:hover {
          color: #156B7E; /* darker teal */
      }

/* 2. EXTERNAL LINKS (Achievement Gold) */
      .page__content a[href^="http"] {
          color: #C59200; /* Deep Metallic Gold */
          text-decoration: none;
          font-weight: 700; /* Extra bold for "heavy" gold feel */
      }
      .page__content a[href^="http"]:hover {
          color: #997000; /* Burnished Bronze / Darker Gold */
          background-color: #FFF9E5; /* Pale Gold Tint */
      }

      .page__content h2 {
        font-size: 1.05em;
        border-bottom: 1px solid #bbbbbb;
      }

      .page__content h1 {

        font-size: 2em;
        border-bottom: 1px solid #bbbbbb;

      }

      /* Update the selector to target the class directly (which is now on a div) */
      .run-in-section {
          margin-top: 1.5em; 
      }

      p {
          text-indent: 2em;
          margin-top: 0em !important;
          margin-bottom: 0.5em !important;
          text-align: justify;
          text-justify: inter-word;
      }
      .run-in-section p {
          margin: 0;
          text-indent: 0em;
      }
    </style>
  </head>
  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://afaji.github.io/">Alham Fikri Aji</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://scholar.google.ca/citations?hl=en&user=0Cyfqv4AAAAJ&view_op=list_works&sortby=pubdate">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://afaji.github.io/group/">Team</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://afaji.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      <article class="splash" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="Research Statement">
        
        
        
    
        <section class="page__content" itemprop="text">
          <h1 id="research-statement">Research Statement</h1>

<p>NLP technology has progressed significantly over the years. Yet, the
focus is still heavily English-centric, leaving many languages behind.
My research interest focuses on working on NLP for underrepresented
languages. However, the English-centric nature of AI research is not
just about progress in terms of models or data. AI resources are also
heavily distributed to a limited number of communities, leaving compute
resources scarce for many NLP communities. With trends moving towards
large language models, it is even more prohibitive for many communities
to participate in NLP research and deployment.</p>

<p>My research goal can be summarized as “<strong>making NLP technology inclusive
and accessible</strong>”. To achieve this, my technical depth lies in two
synergistic areas: (1) rigorous data-centric methodologies to construct
high-quality benchmarks for low-resource languages, and (2) algorithmic
efficiency to develop lightweight, accessible models. I primarily
publish in *CL venues, maintaining an h-index of 35 and close to 8,000
citations according to <a href="https://www.semanticscholar.org/author/Alham-Fikri-Aji/8129718">Semantic
Scholar</a>.
I have received <a href="https://afaji.github.io/cv/#awards">5 paper awards</a> at
these conferences and was recently honored with the <a href="https://mbzuai.ac.ae/news/mbzuai-celebrates-faculty-excellence-at-annual-recognition-reception/">2025 MBZUAI Early
Career Researcher
Award</a>,
which recognizes assistant professors with exceptional research promise.</p>

<h2 id="multilingual-and-cultural-nlp">Multilingual and Cultural NLP</h2>

<p>Most of my recent work focuses on multilingual and culturally grounded
NLP, covering various topics from resource building to interpretability.
Most of my research awards also fall into this area of work.</p>

<div class="run-in-section">

  <p><strong>Multilingual NLP Resources and Benchmarks</strong> A persistent challenge in
multilingual NLP is the scarcity of high‑quality datasets for both
training and nuanced evaluation. My current primary area of depth lies
in multilingual and culturally grounded data construction. I address the
scarcity of high-quality data not merely by collection, but by
developing robust, high-quality methodologies for resource building and
evaluation. This involves designing protocols for human annotation,
quality control, and cultural relevance. Some resources that I worked on
are highlighted below.</p>

</div>

<div id="tab:resources">

<table>
<caption>Overview of contributed multilingual datasets and
resources.</caption>
<thead>
<tr>
<th style="text-align: left;"><strong>Resource</strong></th>
<th style="text-align: left;"><strong>Description &amp;
Scope</strong></th>
<th style="text-align: left;"><strong>Ref.</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/IndoNLP/indonli">IndoNLI</a></td>
<td style="text-align: left;">Natural Language Inference (NLI) for
Indonesian.</td>
<td style="text-align: left;"><a class="citation" href="#mahendra-etal-2021-indonli">(Mahendra et al., 2021)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/amazon_science/mintaka">Mintaka</a></td>
<td style="text-align: left;">Complex Question Answering across 9
languages.</td>
<td style="text-align: left;"><a class="citation" href="#sen-etal-2022-mintaka">(Sen et al., 2022)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/IndoNLP/nusax_senti">NusaX</a></td>
<td style="text-align: left;">Sentiment analysis and MT covering 10
Indonesian local languages.</td>
<td style="text-align: left;"><a class="citation" href="#winata-etal-2023-nusax">(Winata et al., 2023)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/IndoNLP/NusaWrites">NusaWrites</a></td>
<td style="text-align: left;">Generation benchmarks for 12 Indonesian
languages.</td>
<td style="text-align: left;"><a class="citation" href="#cahyawijaya-etal-2023-nusawrites">(Cahyawijaya et al., 2023)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/SEACrowd">SEACrowd</a></td>
<td style="text-align: left;">Multilingual multimodal data hub and
benchmark suite for Southeast Asian languages.</td>
<td style="text-align: left;"><a class="citation" href="#lovenia-etal-2024-seacrowd">(Lovenia et al., 2024)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/SemRel/SemRel2024">SemRel</a></td>
<td style="text-align: left;">Semantic relatedness of Asian and African
languages.<br />
<em>Powered <a href="https://semantic-textual-relatedness.github.io">SemEval-2024 Task
1</a>. (163 participants).</em></td>
<td style="text-align: left;"><a class="citation" href="#ousidhoum-etal-2024-semrel2024">(Ousidhoum et al., 2024)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/mbzuai-nlp/M4">M4</a></td>
<td style="text-align: left;">Multilingual machine-generated text
detection.<br />
<em>Powered <a href="https://aclanthology.org/2024.semeval-1.279/">SemEval-2024 Task
8</a>. (285 participants).</em></td>
<td style="text-align: left;"><a class="citation" href="#wang-etal-2024-m4">(Wang et al., 2024)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/afaji/cvqa">CVQA</a></td>
<td style="text-align: left;">Culturally diverse multilingual Visual
Question Answering of 39 language-country pairs.</td>
<td style="text-align: left;"><a class="citation" href="#romero-etal-2024-cvqa">(Romero et al., 2024)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/haryoaw/COPAL">COPAL-ID</a></td>
<td style="text-align: left;">Culturally specific causal reasoning for
Indonesian.</td>
<td style="text-align: left;"><a class="citation" href="#wibowo-etal-2024-copal">(Wibowo et al., 2024)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/StingrayBench/StingrayBench">Stingray</a></td>
<td style="text-align: left;">Multilingual word-sense disambiguation
benchmark.</td>
<td style="text-align: left;"><a class="citation" href="#cahyawijaya-etal-2025-stingray">(Cahyawijaya et al., 2025)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://brighter-dataset.github.io/">BRIGHTER</a></td>
<td style="text-align: left;">Emotion classification for low-resource
languages.<br />
<em>Powered <a href="https://arxiv.org/abs/2504.17307">SemEval-2025 Task
11</a> (800 participants).</em></td>
<td style="text-align: left;"><a class="citation" href="#muhammad-etal-2025-brighter">(Muhammad et al., 2025)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd">WangchanThai</a></td>
<td style="text-align: left;">Instruction-following dataset for Thai
culture and domains.</td>
<td style="text-align: left;"><a class="citation" href="#limkonchotiwat-etal-2025-wangchanthaiinstruct">(Limkonchotiwat et al., 2025)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/NusaAksara/NusaAksara">NusaAksara</a></td>
<td style="text-align: left;">OCR and translation benchmark for
Indonesian languages in local scripts.</td>
<td style="text-align: left;"><a class="citation" href="#adilazuarda-etal-2025-nusaaksara">(Adilazuarda et al., 2025)</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="https://huggingface.co/datasets/google/loraxbench">LoraxBench</a></td>
<td style="text-align: left;">Multitask benchmark for 20 low-resource
Indonesian languages.</td>
<td style="text-align: left;"><a class="citation" href="#aji-cohn-2025-loraxbench">(Aji and Cohn, 2025)</a></td>
</tr>
</tbody>
</table>

</div>

<div class="run-in-section">

  <p><strong>Culturally‑Nuanced NLP</strong> Beyond language coverage, my work examines
cultural representation and evaluation, since covering languages
themselves might not be enough. This is an issue with translated
benchmarks, in which you get questions that are not relevant to the
local context, even if the language is translated. Several of our
benchmarks mentioned earlier cover not only language aspects but also
cultural understanding.</p>

</div>

<p>In <a href="#adilazuarda-etal-2024-towards">Adilazuarda et al. (2024)</a>, we survey the research
on culture in large language models and find that most studies use
narrow proxies such as demographics or semantics without defining
culture itself. We propose a taxonomy of these approaches and highlight
gaps in contextual and robust evaluations of cultural representation.</p>

<p>In <a href="#adilazuarda-etal-2025-surveys">Adilazuarda et al. (2025)</a>, we explore how to adapt
large language models to better reflect diverse cultural values by
moving beyond survey-based data such as the World Values Survey (WVS).
Our results show that this mixed-source approach produces more
culturally distinct and balanced models than relied on survey data
alone.</p>

<div class="run-in-section">

  <p><strong>Indonesian NLP</strong> Having grown up with Indonesian languages and
cultures, some of my work deeply involves Indonesian languages. As we
reported in <a class="citation" href="#aji-etal-2022-one">(Aji et al., 2022)</a>, Indonesia is one of the most
culturally and linguistically diverse countries, with over 700 languages
spoken and more than 200M population. Yet, NLP research for Indonesian
languages is underrepresented. We present challenges and opportunities
for Indonesian NLP. This work is widely cited as a reference in
Indonesian NLP studies.</p>

</div>

<p>In another work in <a class="citation" href="#adilazuarda-etal-2025-nusaaksara">(Adilazuarda et al., 2025)</a>, we
studied current issues with regard to models that are not capable of
dealing with Indonesian native scripts, while releasing a benchmark.
Similarly, in <a class="citation" href="#farhansyah-etal-2025-language">(Farhansyah et al., 2025)</a>, we studied
various Javanese honorific systems in several models, showing that many
models face challenges.</p>

<p>In a collaborative effort with the Indonesian NLP community, we built
<a href="https://github.com/IndoNLP/nusa-crowd">NusaCrowd</a>, a resource catalog
that standardizes NLP resources for Indonesian
languages <a class="citation" href="#cahyawijaya-etal-2023-nusacrowd">(Cahyawijaya et al., 2023)</a>. I was part of the
core team that initiated and designed the project from the very
beginning. NusaCrowd gained more than <strong>270 stars on GitHub</strong>. A
follow-up for South-East Asian languages,
SEACrowd <a class="citation" href="#lovenia-etal-2024-seacrowd">(Lovenia et al., 2024)</a>, was also released; I
similarly served as a core initiator for this expansion, which served as
the embryo of the <a href="https://seacrowd.org">SEA-NLP community</a> of the same
name, in which I am now a member of the advisory board.</p>

<div class="run-in-section">

  <p><strong>Code Switching and Code Mixing</strong> Code-Switching (CS) or Code-Mixing
(CM) is a phenomenon commonly observed in multilingual cultures, making
it inline to my research direction.
In <a href="#winata-etal-2023-decades">Winata et al. (2023)</a>, we provide a systematic
survey of code-switching research in NLP, tracing its evolution from
linguistic theories to modern machine learning. We analyze decades of
progress to highlight key trends, challenges, and future directions.</p>

</div>

<p>Shortly after the release of ChatGPT, we noted
in <a href="#yong-etal-2023-prompting">Yong et al. (2023)</a> that it struggled to generate
and understand CS/CM, although current models have improved
significantly. This work led to an interview for a
<a href="https://www.wired.com/story/chatgpt-non-english-languages-ai-revolution/">Wired</a>
article on the multilingual limitations of early ChatGPT, specifically
in South-East Asia. In <a href="#cahyawijaya-etal-2025-stingray">Cahyawijaya et al. (2025)</a>, we
find that multilingual LLMs consistently fail to distinguish the
meanings of false friends across languages, revealing major gaps in
cross-lingual sense understanding.</p>

<div class="run-in-section">

  <p><strong>Multilingual LLMs</strong> In parallel with dataset and benchmark building, I
collaborate on building multilingual models, including
<a href="https://huggingface.co/bigscience/mt0-base">mT0</a> and
<a href="https://huggingface.co/bigscience/bloomz">BLOOMZ</a> <a class="citation" href="#muennighoff-etal-2023-crosslingual">(Muennighoff et al., 2023)</a>,
the Arabic‑centric
<a href="https://huggingface.co/inceptionai/jais-13b">Jais</a> <a class="citation" href="#sengupta2023jais">(Sengupta et al., 2023)</a>,
and the Indonesian LLM
<a href="https://huggingface.co/indonlp/cendol">Cendol</a> <a class="citation" href="#cahyawijaya-etal-2024-cendol">(Cahyawijaya et al., 2024)</a>.
Particularly, BLOOMZ attracted significant traction and gained decent
citations and downloads, with more than <strong>1M downloads</strong> of all time,
and it is still widely downloaded now. Since joining Google as a
visiting researcher, I have also worked on the multilinguality aspect of
Gemini.</p>

</div>

<div class="run-in-section">

  <p><strong>Interpretability and Understanding of Multilingual Models</strong> Recently,
I explored the intersection of multilinguality and interpretability. In
<a class="citation" href="#rahmanisa2025unveiling">(Rahmanisa et al., 2025)</a>, we find that amplifying
language-specific neurons in multilingual models boosts performance in
their respective languages, particularly low-resource ones, but often
harms cross-lingual generalization. Separately, in
<a class="citation" href="#andrylie2025sparse">(Andrylie et al., 2025)</a> we use sparse auto encoders to identify
interpretable neurons associated with particular languages, showing that
multilingual models encode clear language-specific representations
within their internal layers.</p>

</div>

<h2 id="lightweight-nlp-systems">Lightweight NLP Systems</h2>

<div class="run-in-section">

  <p><strong>Fast Machine Translation System</strong> During my PhD, I worked on fast
machine translation systems. In
<a class="citation" href="#aji-heafield-2020-compressing">(Aji and Heafield, 2020)</a>, we explored quantization
techniques for neural machine translation and achieved 4-bit precision
using a log-based quantization approach. Building on that, I
collaborated with others in a shared task on efficient machine
translation. By combining quantization, knowledge distillation, and
model pruning, we achieved the best overall
performance <a class="citation" href="#bogoychev-etal-2020-edinburghs">(Bogoychev et al., 2020)</a>. Although I no
longer work exclusively on machine translation, my current research on
lightweight models continues in the same direction.</p>

</div>

<div class="run-in-section">

  <p><strong>Lightweight Models via Distillation</strong> During the early days of GPT, we
distilled ChatGPT into several smaller-sized models smaller than 1B
parameters with, back then, reasonable performance in our
<a href="https://github.com/mbzuai-nlp/LaMini-LM">Lamini-LM</a>
project <a class="citation" href="#wu-etal-2024-lamini">(Wu et al., 2024)</a>. Lamini models are still one of
the most downloaded models in MBZUAI’s HuggingFace repo and gained more
than <strong>800 GitHub stars</strong>. Some of the lightweight model efforts focus
on multilingual capabilities. For example,
<a href="https://huggingface.co/datasets/MBZUAI/Bactrian-X">Bactrian-X</a> is a
distilled multilingual model that covers 52
languages <a class="citation" href="#li2023bactrian">(Li et al., 2023)</a>. We have also attempted to distill a
large multilingual encoder model for low-resource
languages <a class="citation" href="#cruz-2025-extracting">(Cruz, 2025)</a>.</p>

</div>

<div class="run-in-section">

  <p><strong>Sink-Free Attention Transformers</strong> In our ongoing work
<a class="citation" href="#zuhri2025softpick">(Zuhri et al., 2025)</a>, we proposed a softmax replacement named
SoftPick, whose objective is to remove the attention sink. We managed to
remove the attention sink, thus making the attention sparse. With this,
we show that the model can be better quantized, hence improving the
efficiency.</p>

</div>

<div class="run-in-section">

  <p><strong>Knowledge Distillation Study</strong> In <a class="citation" href="#aji-etal-2020-neural">(Aji et al., 2020)</a>, we
show that in neural machine translation transfer learning, copying the
inner layers of a model is essential for quality gains. Our recent work
in <a class="citation" href="#wibowo2025iterabre">(Wibowo et al., 2025)</a> similarly investigates model copying in
knowledge distillation in multilingual settings. We also study the potential harm of knowledge distillation. In
<a class="citation" href="#mansurov-etal-2025-data">(Mansurov et al., 2025)</a>, we find that leaked data (such as
test data) can also be accidentally leaked by knowledge distillation. At
the moment, we are investigating leakage of PIIs or poisoned data via
distillation.</p>

  <h2 id="efficient-training-of-nlp-systems">Efficient Training of NLP Systems</h2>

  <p>Not only inference, but lack of data could also be the issue of
inaccessible NLP systems. Some of my work explores faster or better
learning.</p>

  <div class="run-in-section">

    <p><strong>Effective Language Extension of NLP Models</strong> With the lack of training
data for many languages, we investigate various methods to address this.
In <a class="citation" href="#adilazuarda-etal-2024-lingualchemy">(Adilazuarda et al., 2024)</a>, we enable unseen
generalization of encoder models through an additional loss, in which we
ask the model to learn the language representation vector of the input
and use URIEL vectors as label. This method significantly improves the
performance of some unseen languages, such as Amharic.</p>

  </div>

  <p>In <a class="citation" href="#elshabrawy-etal-2025-statement">(Elshabrawy et al., 2025)</a>, we enable zero-shot
language and task generalization for encoder models by training the
model with true/false statements across languages, enabling ‘prompting’
for encoder models. This project was part of MBZUAI’s 2024 internship
program with talented undergraduate interns visiting for a month. Our
project was awarded the <strong>best team award</strong> among other MBZUAI
internship projects.</p>

  <div class="run-in-section">

    <p><strong>Multi-Token Learning</strong> An ongoing work in
<a class="citation" href="#zuhri2025predicting">(Zuhri et al., 2025)</a>, we proposed a new learning objective to
learn from multiple tokens at once, with the aim of better training the
model. Specifically, we instruct the model to predict token ordering.</p>

  </div>

  <h2 id="multimodal-multicultural-nlp">Multimodal-Multicultural NLP</h2>

  <p>Most of my work has focused purely on text. With the advancement of AI
technology going beyond text, multimodality is the next direction that
fits my overarching goal that I recently explored.</p>

  <div class="run-in-section">

    <p><strong>Multimodal-Multicultural Datasets and Benchmarks</strong> I have been working
on data set construction for a while; hence, multimodal datasets were a
natural extension. <a href="https://huggingface.co/datasets/afaji/cvqa">CVQA</a> is
one of the largest human-made multimodal multilingual datasets. I served
as the primary lead and organizer of this initiative, conceptualizing
the project and spearheading a massive collaboration of over 70 authors
to construct culturally relevant visual question answering for more than
30 language and country pairs.</p>

  </div>

  <p>In <a class="citation" href="#winata-etal-2025-worldcuisines">(Winata et al., 2025)</a>, where I served as a senior
advisor working closely with the core team, we gather images of food and
cuisine from around the world and annotate them. A recent follow-up work
on that in <a class="citation" href="#irawan2025visionlanguagemodelsconfused">(Irawan et al., 2025)</a>, in which
we perform adversarial image editing by replacing the background with
landmarks of different countries, or by adding flags of different
countries, noted that VLMs are easily distracted.</p>

  <div class="run-in-section">

    <p><strong>Multimodal-Multicultural Models</strong> In an ongoing project with the
SEACrowd community, we are building
<a href="https://seacrowd.org/projects/2025-seavl-phase-2">SeaVL</a>, a multimodal
language model for Southeast Asian people. We started with the data
set <a class="citation" href="#cahyawijaya-etal-2025-crowdsource">(Cahyawijaya et al., 2025)</a> and now work on the
model. Another ongoing project focuses on building multilingual,
multimodal reward models.</p>

  </div>

  <h2 id="human-computer-interaction-of-nlp-systems">Human-Computer Interaction of NLP Systems</h2>

  <p>I have recently initiated a new line of research into Human-AI
interaction. This is particularly important for inclusive technology, as
different demographics and cultural backgrounds significantly influence
how users perceive and expect AI to behave.</p>

  <div class="run-in-section">

    <p><strong>Bias in Human Preferences</strong> In <a href="#wu-aji-2025-style">Wu and Aji (2025)</a>, we
explored typical human-preference evaluations used in standard
leaderboards. We noted that humans exhibit a bias towards output length
and grammatical correctness to such a degree that they often prefer
hallucinated outputs, provided they are long and grammatically polished.</p>

  </div>

  <p>In our follow-up work
<a class="citation" href="#chevi-2025-individual">(Chevi et al., 2025)</a>, we found that this preference
correlates with the user’s personality traits. Specifically, users with
different personality profiles prioritize distinct aspects of model
responses, suggesting that a single universal reward model is
insufficient to capture the diversity of human preferences.</p>

  <div class="run-in-section">

    <p><strong>AI Literacy in Education</strong> In ongoing work, we are exploring the AI
literacy of Indonesian teachers and their impact on teaching pedagogy.
We aim to uncover the current level of AI literacy in Indonesia and
provide recommendations to policymakers to ensure AI is used effectively
and appropriately in classroom activities.</p>

  </div>

  <h2 id="future-research-agenda">Future Research Agenda</h2>

  <p>My long-term goal remains to democratize NLP technology. Having
established strong foundations in data-centric NLP and model efficiency,
my next phase focuses on converging these streams into a unified
framework for accessible and inclusive AI technology.</p>

  <div class="run-in-section">

    <p><strong>From Static Resources to Dynamic Simulation.</strong> While my previous work
established static benchmarks, the future of evaluation lies in dynamic
environments. I aim to transition from fixed datasets to interactive
simulations and games. By utilizing scenarios where models engage in
culturally-grounded games, role-play, or debates, we can create
self-evolving benchmarks that resist contamination. Furthermore, this
simulation-based approach will serve as a data synthesis engine,
generating high-quality training signals for underrepresented languages
where natural data is scarce.</p>

  </div>

  <div class="run-in-section">

    <p><strong>Operationalizing Efficient Cultural Multimodality.</strong> The move toward
multimodal models comes with a significant increase in cost. These
models are significantly more resource-intensive than text-only
baselines, making them prohibitive for many communities to utilize.
Furthermore, they are extremely data-hungry, exacerbating the challenge
for low-resource cultures where paired visual-linguistic data is
exceptionally scarce. To bridge this, I will connect my efficiency
research, both training and deployment efficiency for multimodality. My
goal is to develop methods that maximize learning from scarce signals
while reducing the computational burden, ensuring that systems capable
of capturing complex, cultural visual nuances remain accessible to train
and deploy on consumer-grade hardware.</p>

  </div>

  <div class="run-in-section">

    <p><strong>Deepening Cross-Cultural Human-Computer Interaction.</strong> Moving beyond
preliminary preference analysis, I plan to establish a rigorous HCI
research agenda focused on the global user experience. Rather than
purely optimizing model parameters, I aim to conduct empirical studies
on how cultural backgrounds and diverse demographics shape mental
models, trust, and interaction patterns with AI systems. By
investigating these dynamics through a user-centric lens, I seek to
uncover how distinct communities perceive and utilize AI, providing the
foundational insights needed to design interfaces and workflows that are
truly intuitive and inclusive for a global population.</p>

  </div>

  <div class="page-break"></div>

  <h2 id="reference">Reference</h2>

  <ol class="bibliography"><li><div class="csl-entry" id="aji-etal-2020-neural">
  Aji, Alham Fikri and Bogoychev, Nikolay and Heafield, Kenneth and Sennrich, Rico. 2020.
  <a href="https://aclanthology.org/2020.acl-main.688"><strong>In Neural Machine Translation, What Does Transfer Learning Transfer?</strong></a>.
  
    <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="aji-heafield-2020-compressing">
  Aji, Alham Fikri and Heafield, Kenneth. 2020.
  <a href="https://aclanthology.org/2020.ngt-1.4"><strong>Compressing Neural Machine Translation Models with 4-bit Precision</strong></a>.
  
    <em>Proceedings of the Fourth Workshop on Neural Generation and Translation</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="bogoychev-etal-2020-edinburghs">
  Bogoychev, Nikolay and Grundkiewicz, Roman and Aji, Alham Fikri and Behnke, Maximiliana and Heafield, Kenneth and Kashyap, Sidharth and Farsarakis, Emmanouil-Ioannis and Chudyk, Mateusz. 2020.
  <a href="https://aclanthology.org/2020.ngt-1.26"><strong>Edinburgh’s Submissions to the 2020 Machine Translation Efficiency Task</strong></a>.
  
    <em>Proceedings of the Fourth Workshop on Neural Generation and Translation</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="mahendra-etal-2021-indonli">
  Mahendra, Rahmad and Aji, Alham Fikri and Louvan, Samuel and Rahman, Fahrurrozi and Vania, Clara. 2021.
  <a href="https://aclanthology.org/2021.emnlp-main.821"><strong>IndoNLI: A Natural Language Inference Dataset for Indonesian</strong></a>.
  
    <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="aji-etal-2022-one">
  Aji, Alham Fikri and Winata, Genta Indra and Koto, Fajri and Cahyawijaya, Samuel and Romadhony, Ade and Mahendra, Rahmad and Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and Baldwin, Timothy and Lau, Jey Han and Ruder, Sebastian. 2022.
  <a href="https://aclanthology.org/2022.acl-long.500"><strong>One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia</strong></a>.
  
    <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="sen-etal-2022-mintaka">
  Sen, Priyanka and Aji, Alham Fikri and Saffari, Amir. 2022.
  <a href="https://aclanthology.org/2022.coling-1.138"><strong>Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering</strong></a>.
  
    <em>Proceedings of the 29th International Conference on Computational Linguistics</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cahyawijaya-etal-2023-nusacrowd">
  Cahyawijaya, Samuel and Lovenia, Holy and Aji, Alham Fikri and Winata, Genta Indra and Wilie, Bryan and Mahendra, Rahmad and Wibisono, Christian and Romadhony, Ade and Vincentio, Karissa and Koto, Fajri and Santoso, Jennifer and Moeljadi, David and Wirawan, Cahya and Hudi, Frederikus and Parmonangan, Ivan Halim and Alfina, Ika and Wicaksono, Muhammad Satrio and Putra, Ilham Firdausi and others. 2023.
  <a href="https://aclanthology.org/2023.findings-acl.868"><strong>NusaCrowd: Open Source Initiative for Indonesian NLP Resources</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: ACL 2023</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cahyawijaya-etal-2023-nusawrites">
  Cahyawijaya, Samuel and Lovenia, Holy and Koto, Fajri and Adhista, Dea and Dave, Emmanuel and Oktavianti, Sarah and Akbar, Salsabil and Lee, Jhonson and Shadieq, Nuur and Cenggoro, Tjeng Wawan and Linuwih, Hanung Wahyuning and Wilie, Bryan and Muridan, Galih Pradipta and Winata, Genta Indra and Moeljadi, David and Aji, Alham Fikri and Purwarianti, Ayu and Fung, Pascale. 2023.
  <a href="https://aclanthology.org/2023.ijcnlp-main.60"><strong>NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages</strong></a>.
  
    <em>Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="li2023bactrian">
  Li, Haonan and Koto, Fajri and Wu, Minghao and Aji, Alham Fikri and Baldwin, Timothy. 2023.
  <a href=""><strong>Bactrian-x: Multilingual replicable instruction-following models with low-rank adaptation</strong></a>.
  
    <em>arXiv preprint arXiv:2305.15011</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="muennighoff-etal-2023-crosslingual">
  Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin. 2023.
  <a href="https://aclanthology.org/2023.acl-long.891"><strong>Crosslingual Generalization through Multitask Finetuning</strong></a>.
  
    <em>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="sengupta2023jais">
  Sengupta, Neha and Sahu, Sunil Kumar and Jia, Bokang and Katipomu, Satheesh and Li, Haonan and Koto, Fajri and Afzal, Osama Mohammed and Kamboj, Samta and Pandit, Onkar and Pal, Rahul and Pradhan, Lalit and Mujahid, Zain Muhammad and Baali, Massa and Aji, Alham Fikri and Liu, Zhengzhong and Hock, Andy and Feldman, Andrew and Lee, Jonathan and Jackson, Andrew and Nakov, Preslav and Baldwin, Timothy and Xing, Eric. 2023.
  <a href="https://arxiv.org/abs/2308.16149"><strong>Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wibowo2023language">
  Wibowo, Haryo Akbarianto and Aji, Alham Fikri and Wijaya, Derry Tanti. 2023.
  <a href="https://www.researchgate.net/publication/389510799_Do_Language_Models_Understand_Honorific_Systems_in_Javanese"><strong>Do Language Models Understand Honorific Systems in Javanese?</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="winata-etal-2023-decades">
  Winata, Genta and Aji, Alham Fikri and Yong, Zheng Xin and Solorio, Thamar. 2023.
  <a href="https://aclanthology.org/2023.findings-acl.185"><strong>The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: ACL 2023</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="winata-etal-2023-nusax">
  Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya, Samuel and Mahendra, Rahmad and Koto, Fajri and Romadhony, Ade and Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and Fung, Pascale and Baldwin, Timothy and Lau, Jey Han and Sennrich, Rico and Ruder, Sebastian. 2023.
  <a href="https://aclanthology.org/2023.eacl-main.57"><strong>NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages</strong></a>.
  
    <em>Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="yong-etal-2023-prompting">
  Yong, Zheng Xin and Zhang, Ruochen and Forde, Jessica and Wang, Skyler and Subramonian, Arjun and Lovenia, Holy and Cahyawijaya, Samuel and Winata, Genta and Sutawika, Lintang and Cruz, Jan Christian Blaise and Tan, Yin Lin and Phan, Long and Phan, Long and Garcia, Rowena and Solorio, Thamar and Aji, Alham Fikri. 2023.
  <a href="https://aclanthology.org/2023.calcs-1.5/"><strong>Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages</strong></a>.
  
    <em>Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="adilazuarda-etal-2024-lingualchemy">
  Adilazuarda, Muhammad Farid and Cahyawijaya, Samuel and Winata, Genta Indra and Purwarianti, Ayu and Aji, Alham Fikri. 2024.
  <a href="https://aclanthology.org/2024.findings-emnlp.225"><strong>LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: EMNLP 2024</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="adilazuarda-etal-2024-towards">
  Adilazuarda, Muhammad Farid and Mukherjee, Sagnik and Lavania, Pradhyumna and Singh, Siddhant Shivdutt and Aji, Alham Fikri and O’Neill, Jacki and Modi, Ashutosh and Choudhury, Monojit. 2024.
  <a href="https://aclanthology.org/2024.emnlp-main.882"><strong>Towards Measuring and Modeling “Culture” in LLMs: A Survey</strong></a>.
  
    <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cahyawijaya-etal-2024-cendol">
  Cahyawijaya, Samuel and Lovenia, Holy and Koto, Fajri and Putri, Rifki Afina and Dave, Emmanuel and Lee, Jhonson and Shadieq, Nuur and Cenggoro, Wawan and Akbar, Salsabil Maulana and Mahendra, Muhammad Ihza and Putri, Dea Annisayanti and Wilie, Bryan and Winata, Genta Indra and Aji, Alham Fikri and Purwarianti, Ayu and Fung, Pascale. 2024.
  <a href="https://aclanthology.org/2024.acl-long.796"><strong>Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages</strong></a>.
  
    <em>Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="lovenia-etal-2024-seacrowd">
  Lovenia, Holy and Mahendra, Rahmad and Cahyawijaya, Samuel and Winata, Genta Indra and Aji, Alham Fikri and others. 2024.
  <a href="https://aclanthology.org/2024.emnlp-main.296"><strong>SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages</strong></a>.
  
    <em>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="ousidhoum-etal-2024-semrel2024">
  Ousidhoum, Nedjma and Muhammad, Shamsuddeen and Abdalla, Mohamed and Abdulmumin, Idris and Ahmad, Ibrahim and Ahuja, Sanchit and Aji, Alham and Araujo, Vladimir and Ayele, Abinew and Baswani, Pavan and Beloucif, Meriem and Biemann, Chris and Bourhim, Sofia and Kock, Christine and Dekebo, Genet and Hourrane, Oumaima and Kanumolu, Gopichand and Madasu, Lokesh and Rutunda, Samuel and Shrivastava, Manish and Solorio, Thamar and Surange, Nirmal and Tilaye, Hailegnaw and Vishnubhotla, Krishnapriya and Winata, Genta and Yimam, Seid and Mohammad, Saif. 2024.
  <a href="https://aclanthology.org/2024.findings-acl.147/"><strong>SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 13 Languages</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: ACL 2024</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="romero-etal-2024-cvqa">
  Romero, David and Lyu, Chenyang and Wibowo, Haryo Akbarianto and  ...  and Solorio, Thamar and Aji, Alham Fikri. 2024.
  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/1568882ba1a50316e87852542523739c-Abstract-Datasets_and_Benchmarks_Track.html"><strong>CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark</strong></a>.
  
    <em>Advances in Neural Information Processing Systems 37 (NeurIPS 2024)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wang-etal-2024-semeval">
  Wang, Yuxia and Aji, Alham Fikri and Shelmanov, Artem and Whitehouse, Chenxi and Ivanov, Petar and Mansurov, Jonibek and Su, Jinyan and Mahmoud, Tarek and Afzal, Osama Mohammed and Tsvigun, Akim and Sasaki, Toru and Arnold, Thomas and Habash, Nizar and Gurevych, Iryna and Nakov, Preslav. 2024.
  <a href="https://aclanthology.org/2024.semeval-1.148"><strong>SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection</strong></a>.
  
    <em>Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wang-etal-2024-m4">
  Wang, Yuxia and Mansurov, Jonibek and Ivanov, Petar and Su, Jinyan and Shelmanov, Artem and Tsvigun, Akim and Whitehouse, Chenxi and Mohammed Afzal, Osama and Mahmoud, Tarek and Sasaki, Toru and Arnold, Thomas and Aji, Alham Fikri and Habash, Nizar and Gurevych, Iryna and Nakov, Preslav. 2024.
  <a href="https://aclanthology.org/2024.eacl-long.83/"><strong>M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection</strong></a>.
  
    <em>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wibowo-etal-2024-copal">
  Wibowo, Haryo and Fuadi, Erland and Nityasya, Made and Prasojo, Radityo Eko and Aji, Alham. 2024.
  <a href="https://aclanthology.org/2024.naacl-long.77"><strong>COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances</strong></a>.
  
    <em>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wu-etal-2024-lamini">
  Wu, Minghao and Waheed, Abdul and Zhang, Chiyu and Abdul-Mageed, Muhammad and Aji, Alham Fikri. 2024.
  <a href="https://aclanthology.org/2024.eacl-long.57"><strong>LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions</strong></a>.
  
    <em>Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="adilazuarda-etal-2025-surveys">
  Adilazuarda, Farid and Liu, Chen Cecilia and Gurevych, Iryna and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.emnlp-main.912/"><strong>From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs</strong></a>.
  
    <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="adilazuarda-etal-2025-nusaaksara">
  Adilazuarda, Muhammad Farid and Wijanarko, Musa Izzanardi and Susanto, Lucky and Nur’aini, Khumaisa and Wijaya, Derry Tanti and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.acl-long.1377"><strong>NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts</strong></a>.
  
    <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="aji-cohn-2025-loraxbench">
  Aji, Alham Fikri and Cohn, Trevor. 2025.
  <a href="https://aclanthology.org/2025.emnlp-main.881/"><strong>LORAXBENCH: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages</strong></a>.
  
    <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="andrylie2025sparse">
  Andrylie, Lyzander Marciano and Rahmanisa, Inaya and Ihsani, Mahardika Krisna and Wicaksono, Alfan Farizki and Wibowo, Haryo Akbarianto and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2507.11230"><strong>Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cahyawijaya-etal-2025-crowdsource">
  Cahyawijaya, Samuel and Lovenia, Holy and Moniz, Joel Ruben Antony and Wong, Tack Hwa and Farhansyah, Mohammad Rifqi and Maung, Thant Thiri and Hudi, Frederikus and Anugraha, David and Habibi, Muhammad Ravi Shulthan and Qorib, Muhammad Reza and Agarwal, Amit and Imperial, Joseph Marvin and Patel, Hitesh Laxmichand and Feliren, Vicky and Nasution, Bahrul Ilmi and Rufino, Manuel Antonio and Winata, Genta Indra and Rajagede, Rian Adam and Catalan, Carlos Rafael and Imam, Mohamed Fazli Mohamed and Pattnayak, Priyaranjan and Pranida, Salsabila Zahirah and Pratama, Kevin and Bangera, Yeshil and Na-Thalang, Adisai and Monderin, Patricia Nicole and Song, Yueqi and Simon, Christian and Ng, Lynnette Hui Xian and Sapan, Richardy Lobo and Rafi, Taki Hasan and Wang, Bin and Supryadi and Veerakanjana, Kanyakorn and Ittichaiwong, Piyalitt and Roque, Matthew Theodore and Vincentio, Karissa and Kreangphet, Takdanai and Artkaew, Phakphum and Palgunadi, Kadek Hendrawan and Yu, Yanzhi and Hastuti, Rochana Prih and Nixon, William and Bangera, Mithil and Lim, Adrian Xuan Wei and Khine, Aye Hninn and Zhafran, Hanif Muhammad and Ferdinan, Teddy and Izzani, Audra Aurora and Singh, Ayushman and Evan, Evan and Krito, Jauza Akbar and Anugraha, Michael and Ilasariya, Fenal Ashokbhai and Li, Haochen and Daniswara, John Amadeo and Tjiaranata, Filbert Aurelian and Yulianrifat, Eryawan Presma and Udomcharoenchaikit, Can and Ansori, Fadil Risdian and Ihsani, Mahardika Krisna and Nguyen, Giang and Barik, Anab Maulana and Velasco, Dan John and Genadi, Rifo Ahmad and Saha, Saptarshi and Wei, Chengwei and Flores, Isaiah Edri W. and Han, Kenneth Chen Ko and Santos, Anjela Gail D. and Lim, Wan Shen and Phyo, Kaung Si and Santos, Tim and Dwiastuti, Meisyarah and Luo, Jiayun and Cruz, Jan Christian Blaise and Hee, Ming Shan and Hanif, Ikhlasul Akmal and Hakim, M.Alif Al and Sya’ban, Muhammad Rizky and Kerdthaisong, Kun and Miranda, Lester James Validad and Koto, Fajri and Fatyanosa, Tirana Noor and Aji, Alham Fikri and Rosal, Jostin Jerico and Kevin, Jun and Wijaya, Robert and Kampman, Onno P. and Zhang, Ruochen and Karlsson, Börje F. and Limkonchotiwat, Peerat. 2025.
  <a href="https://aclanthology.org/2025.acl-long.916/"><strong>Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia</strong></a>.
  
    <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cahyawijaya-etal-2025-stingray">
  Cahyawijaya, Samuel and Zhang, Ruochen and Cruz, Jan Christian Blaise and Lovenia, Holy and Gilbert, Elisa and Nomoto, Hiroki and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.findings-naacl.178"><strong>Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Senses</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: NAACL 2025</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="chevi-2025-individual">
  Chevi, Rendi and Inui, Kentaro and Solorio, Thamar and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2504.17083"><strong>How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="cruz-2025-extracting">
  Cruz, Jan Christian Blaise. 2025.
  <a href="https://aclanthology.org/2025.loreslm-1.17/"><strong>Extracting General-use Transformers for Low-resource Languages via Knowledge Distillation</strong></a>.
  
    <em>Proceedings of the First Workshop on Language Models for Low-Resource Languages</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="elshabrawy-etal-2025-statement">
  Elshabrawy, Ahmed and Nguyen, Thanh-Nhi and Kang, Yeeun and Feng, Lihan and Jain, Annant and Shaikh, Faadil Abdullah and Mansurov, Jonibek and Imam, Mohamed Fazli Mohamed and Ortiz-Barajas, Jesus-German and Chevi, Rendi and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.findings-acl.835"><strong>Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models</strong></a>.
  
    <em>Findings of the Association for Computational Linguistics: ACL 2025</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="farhansyah-etal-2025-language">
  Farhansyah, Mohammad Rifqi and Darmawan, Iwan and Kusumawardhana, Adryan and Winata, Genta Indra and Aji, Alham Fikri and Wijaya, Derry Tanti. 2025.
  <a href="https://aclanthology.org/2025.acl-long.1296/"><strong>Do Language Models Understand Honorific Systems in Javanese?</strong></a>.
  
    <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="irawan2025visionlanguagemodelsconfused">
  Irawan, Patrick Amadeus and Hanif, Ikhlasul Akmal and Kautsar, Muhammad Dehan Al and Winata, Genta Indra and Koto, Fajri and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2511.17004"><strong>Vision Language Models are Confused Tourists</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="limkonchotiwat-etal-2025-wangchanthaiinstruct">
  Limkonchotiwat, Peerat and Tuchinda, Pume and Lowphansirikul, Lalita and Nonesung, Surapon and Tasawong, Panuthep and Aji, Alham Fikri and Udomcharoenchaikit, Can and Nutanong, Sarana. 2025.
  <a href="https://aclanthology.org/2025.emnlp-main.175/"><strong>WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai</strong></a>.
  
    <em>Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="mansurov-etal-2025-data">
  Mansurov, Jonibek and Sakip, Akhmed and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.acl-long.407"><strong>Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation</strong></a>.
  
    <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="muhammad-etal-2025-brighter">
  Muhammad, Shamsuddeen Hassan and Ousidhoum, Nedjma and Abdulmumin, Idris and Wahle, Jan Philip and Ruas, Terry and Beloucif, Meriem and de Kock, Christine and Surange, Nirmal and Teodorescu, Daniela and Ahmad, Ibrahim Said and Adelani, David Ifeoluwa and Aji, Alham Fikri and Ali, Felermino D. M. A. and Alimova, Ilseyar and Araujo, Vladimir and Babakov, Nikolay and Baes, Naomi and Bucur, Ana-Maria and Bukula, Andiswa and Cao, Guanqun and Tufiño, Rodrigo and Chevi, Rendi and Chukwuneke, Chiamaka Ijeoma and Ciobotaru, Alexandra and Dementieva, Daryna and Gadanya, Murja Sani and Geislinger, Robert and Gipp, Bela and Hourrane, Oumaima and Ignat, Oana and Lawan, Falalu Ibrahim and Mabuya, Rooweither and Mahendra, Rahmad and Marivate, Vukosi and Panchenko, Alexander and Piper, Andrew and Ferreira, Charles Henrique Porto and Protasov, Vitaly and Rutunda, Samuel and Shrivastava, Manish and Udrea, Aura Cristina and Wanzare, Lilian Diana Awuor and Wu, Sophie and Wunderlich, Florian Valentin and Zhafran, Hanif Muhammad and Zhang, Tianhui and Zhou, Yi and Mohammad, Saif M.. 2025.
  <a href="https://aclanthology.org/2025.acl-long.436/"><strong>BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages</strong></a>.
  
    <em>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="muhammad-etal-2025-semeval">
  Muhammad, Shamsuddeen Hassan and Ousidhoum, Nedjma and Abdulmumin, Idris and Yimam, Seid Muhie and Wahle, Jan Philip and Lima Ruas, Terry and Beloucif, Meriem and De Kock, Christine and Belay, Tadesse Destaw and Ahmad, Ibrahim Said and Surange, Nirmal and Teodorescu, Daniela and Adelani, David Ifeoluwa and Aji, Alham Fikri and Ali, Felermino Dario Mario and Araujo, Vladimir and Ayele, Abinew Ali and Ignat, Oana and Panchenko, Alexander and Zhou, Yi and Mohammad, Saif. 2025.
  <a href="https://aclanthology.org/2025.semeval-1.327/"><strong>SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection</strong></a>.
  
    <em>Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="rahmanisa2025unveiling">
  Rahmanisa, Inaya and Andrylie, Lyzander Marciano and Ihsani, Mahardika Krisna and Wicaksono, Alfan Farizki and Wibowo, Haryo Akbarianto and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2507.22581"><strong>Unveiling the Influence of Amplifying Language-Specific Neurons</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wibowo2025iterabre">
  Wibowo, Haryo Akbarianto and Song, Haiyue and Tanaka, Hideki and Utiyama, Masao and Aji, Alham Fikri and Dabre, Raj. 2025.
  <a href="https://arxiv.org/abs/2503.06291"><strong>IteRABRe: Iterative Recovery-Aided Block Reduction</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="winata-etal-2025-worldcuisines">
  Winata, Genta Indra and Hudi, Frederikus and Irawan, Patrick Amadeus and Anugraha, David and Putri, Rifki Afina and Wang, Yutong and Nohejl, Adam and Prathama, Ubaidillah Ariq and Ousidhoum, Nedjma and Amriani, Afifa and Rzayev, Anar and Das, Anirban and Pramodya, Ashmari and Adila, Aulia and Wilie, Bryan and Mawalim, Candy Olivia and Cheng, Ching Lam and Abolade, Daud and Chersoni, Emmanuele and Santus, Enrico and Ikhwantri, Fariz and Kuwanto, Garry and Zhao, Hanyang and Wibowo, Haryo Akbarianto and Lovenia, Holy and Cruz, Jan Christian Blaise and Putra, Jan Wira Gotama and Myung, Junho and Susanto, Lucky and Machin, Maria Angelica Riera and Zhukova, Marina and Anugraha, Michael and Adilazuarda, Muhammad Farid and Santosa, Natasha and Limkonchotiwat, Peerat and Dabre, Raj and Audino, Rio Alexander and Cahyawijaya, Samuel and Zhang, Shi-Xiong and Salim, Stephanie Yulia and Zhou, Yi and Gui, Yinxuan and Adelani, David Ifeoluwa and Lee, En-Shiun Annie and Okada, Shogo and Purwarianti, Ayu and Aji, Alham Fikri and Watanabe, Taro and Wijaya, Derry Tanti and Oh, Alice and Ngo, Chong-Wah. 2025.
  <a href="https://aclanthology.org/2025.naacl-long.167"><strong>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</strong></a>.
  
    <em>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="wu-aji-2025-style">
  Wu, Minghao and Aji, Alham Fikri. 2025.
  <a href="https://aclanthology.org/2025.coling-main.21/"><strong>Style Over Substance: Evaluation Biases for Large Language Models</strong></a>.
  
    <em>Proceedings of the 31st International Conference on Computational Linguistics</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="zuhri2025predicting">
  Zuhri, Zayd M. K. and Fuadi, Erland Hilman and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2508.19228"><strong>Predicting the Order of Upcoming Tokens Improves Language Modeling</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li>
<li><div class="csl-entry" id="zuhri2025softpick">
  Zuhri, Zayd M. K. and Fuadi, Erland Hilman and Aji, Alham Fikri. 2025.
  <a href="https://arxiv.org/abs/2504.20966"><strong>Softpick: No Attention Sink, No Massive Activations with Rectified Softmax</strong></a>.
  
    <em>Preprint</em>
  .
</div> 
</li></ol>
</div>

        </section>
      </article>
    </div>

    <div class="page__footer">
      <footer>
      </footer>
    </div>

    <script src="https://afaji.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>




<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Research Statement - Alham Fikri Aji</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alham Fikri Aji">
<meta property="og:title" content="Research Statement">


  <link rel="canonical" href="https://afaji.github.io/_pages/research_statement/">
  <meta property="og:url" content="https://afaji.github.io/_pages/research_statement/">





  <meta name="twitter:site" content="@alhamfikri">
  <meta name="twitter:title" content="Research Statement">
  <meta name="twitter:description" content="NLP scientist - Assistant Professor at MBZUAI &amp; Monash Indonesia">
  <meta name="twitter:url" content="https://afaji.github.io/_pages/research_statement/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alham Fikri Aji",
      "url" : "https://afaji.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://afaji.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alham Fikri Aji Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://afaji.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://afaji.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://afaji.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://afaji.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://afaji.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://afaji.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://afaji.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://afaji.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://afaji.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://afaji.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://afaji.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://afaji.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://afaji.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://afaji.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://afaji.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://afaji.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

    <style>
      @media print {
        .page-break {
            page-break-before: always;
        }
      }

      @media screen {
        /* Global font reset to ensure consistency */
        body, p, li, td, th {
          font-size: 0.9rem !important;
        }
      }
      @media print {
          body {
              font-size: 55%;
              -webkit-print-color-adjust: exact;
              print-color-adjust: exact; /* Standard property for non-webkit browsers */
          }
      }

      @media print {
        #main {
          padding-left: 5em !important;
          padding-right: 3em !important;
        }
      }

      #main {
        padding-left: 4em;
        padding-right: 2em;
      }

      ul li {
        margin-bottom: 0.5em;
        margin-top: 0.2em;
        list-style-type: circle;
      }
      
      .compact-ul ul li,
      #awards + ul li,
      #professional-activity + * + ul li {
        margin-bottom: 0.1em;
        margin-top: 0.0em;
        list-style-type: circle;
      }

      .page__content h1, .page__content h2 {
        font-size: 1.05em;
        border-bottom: 1px solid #bbbbbb;
      }

      
.default-author {
    list-style-type: '\25CB' !important; /* Hollow circle */
    padding-left: 0.8em;
}

.last-author {
    list-style-type: '\25CF' !important; /* Full circle */
    padding-left: 0.8em;

}

.first-author {
    list-style-type: '\25A0' !important; /* Full block */
    padding-left: 0.8em;

}


.default-author::marker {
    font-size: 0.7em; /* Bigger marker */
    color: black; /* Marker color */
}


.last-author::marker {
    font-size: 0.7em; /* Bigger marker */
    color: orange; /* Marker color */
}


.first-author::marker {
    font-size: 0.7em; /* Bigger marker */
    color: teal; /* Marker color */
}



      
      
     
    </style>
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://afaji.github.io/">Alham Fikri Aji</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://scholar.google.ca/citations?hl=en&user=0Cyfqv4AAAAJ&view_op=list_works&sortby=pubdate">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://afaji.github.io/group/">Team</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://afaji.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      <article class="splash" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="Research Statement">
        
        
        
    
        <section class="page__content" itemprop="text">
          <p>#Research Statement</p>

<p>NLP technology has progressed significantly over the years. Yet, the
focus is still heavily English-centric, leaving many languages behind.
My research interest focuses on working on NLP for underrepresented
languages. However, the English-centric nature of AI research is not
just about progress in terms of models or data. AI resources are also
heavily distributed to a limited number of communities, leaving compute
resources scarce for many NLP communities. With trends moving towards
large language models, it is even more prohibitive for many communities
to participate in NLP research and deployment.</p>

<p>My research goal can be summarized as “<strong>making NLP technology inclusive
and accessible</strong>”. Consequently, my research direction focuses on
enabling capabilities across languages and cultures while making the
technology as efficient as possible. I primarily publish in *CL venues,
maintaining an h-index of 35 and close to 8,000 citations according to
<a href="https://www.semanticscholar.org/author/Alham-Fikri-Aji/3446059">Semantic
Scholar</a>.
I have received <a href="LINK_TO_YOUR_AWARDS_PAGE">5 paper awards</a> at these
conferences and was recently honored with the <a href="LINK_TO_MBZUAI_NEWS">2025 MBZUAI Early Career
Researcher Award</a>, which recognizes assistant
professors with exceptional research promise.</p>

<h2 id="multilingual-and-cultural-nlp">Multilingual and Cultural NLP</h2>

<p>Most of my recent work focuses on multilingual and culturally grounded
NLP, covering various topics from resource building to interpretability.
Most of my research awards also fall into this area of work.</p>

<h4 id="multilingual-nlp-resources-and-benchmarks">Multilingual NLP Resources and Benchmarks</h4>

<p>A persistent challenge in multilingual NLP is the scarcity of
high‑quality datasets for both training and nuanced evaluation. Some of
my work addresses this by releasing resources spanning diverse languages
and tasks, including
<a href="https://huggingface.co/datasets/IndoNLP/indonli">IndoNLI</a> (NLI for
Indonesian) (Mahendra et al., 2021),
<a href="https://huggingface.co/datasets/IndoNLP/nusax_senti">NusaX</a> (sentiment
analysis of Indonesian languages) (Winata et al., 2023a),
<a href="https://huggingface.co/datasets/amazon_science/mintaka">Mintaka</a>
(complex QA; 9 languages) (Sen et al., 2022),
<a href="https://huggingface.co/datasets/IndoNLP/NusaWrites">NusaWrites</a> (12
Indonesian languages) (Cahyawijaya et al., 2023b),
<a href="https://huggingface.co/datasets/mbzuai-nlp/M4">M4</a>
(multilingual–multidomain machine‑generated text detection) (Wang et
al., 2024b), <a href="https://brighter-dataset.github.io/">BRIGHTER</a> (emotion
classification of low‑resource languages) (Muhammad et al., 2025a),
<a href="https://huggingface.co/datasets/afaji/cvqa">CVQA</a> (culturally diverse
multilingual VQA) (Romero et al., 2024), and
<a href="https://huggingface.co/datasets/haryoaw/COPAL">COPAL-ID</a> (culturally
specific Indonesian causal reasoning) (Wibowo et al., 2024),
<a href="https://huggingface.co/datasets/StingrayBench/StingrayBench">Stingray</a>
(word-sense disambiguation) (Cahyawijaya et al., 2025b), or NusaAksara
(Indonesian languages OCR) (Adilazuarda et al., 2025b). Several of these
resources have also powered shared tasks, such as <a href="https://aclanthology.org/2024.semeval-1.279/">SemEval‑2024 Task
8</a> (Wang et al., 2024a)
and <a href="https://arxiv.org/abs/2504.17307">SemEval‑2025 Task 11</a> (Muhammad
et al., 2025b), the latter having gained 800 participants.</p>

<h4 id="culturallynuanced-nlp">Culturally‑Nuanced NLP</h4>

<p>Beyond language coverage, my work examines cultural representation and
evaluation. Several of our benchmarks explicitly encode cultural nuance,
such as the aforementioned
<a href="https://huggingface.co/datasets/afaji/cvqa">CVQA</a> and
<a href="https://huggingface.co/datasets/haryoaw/COPAL">COPAL-ID</a>.</p>

<p>In Adilazuarda et al. (2024b), we survey research on culture in large
language models and find that most studies use narrow proxies like
demographics or semantics without defining culture itself. We propose a
taxonomy of these approaches and highlight gaps in contextual and robust
evaluations of cultural representation.</p>

<p>In Adilazuarda et al. (2025a), we explore how to adapt large language
models to better reflect diverse cultural values by moving beyond
survey-based data such as the World Values Survey (WVS). Our results
show that this mixed-source approach produces more culturally distinct
and balanced models than relying on survey data alone.</p>

<h4 id="indonesian-nlp">Indonesian NLP</h4>

<p>Having grown up with Indonesian languages and cultures, some of my work
deeply involves Indonesian languages. In (Aji et al., 2022), we
documented structural and data gaps in Indonesia’s 700+ languages. This
work is widely cited as a reference in Indonesian NLP studies. In
(Adilazuarda et al., 2025b), we studied current issues with regard to
models that are not capable of dealing with Indonesian native scripts,
while releasing a benchmark. In (Farhansyah et al., 2025), we studied
various Javanese honorific systems in several models.</p>

<p>With the Indonesian NLP community, of which I am an active member, we
built NusaCrowd, a resource catalog that standardized NLP resources for
Indonesian languages (<span class="nocase">Cahyawijaya et al.</span>,
2023a). NusaCrowd gained near 300 stars on GitHub. A follow-up for
South-East Asian languages, SEACrowd (<span class="nocase">Lovenia et
al.</span>, 2024), was also released and served as the embryo of the
SEA-NLP community of the same name, in which now I am a member of the
advisory board.</p>

<h4 id="code-switching-and-code-mixing">Code Switching and Code Mixing</h4>

<p>Code-Switching (CS) or Code-Mixing (CM) is a phenomenon commonly
observed in multilingual cultures, making it inline to my research
direction. In Winata et al. (2023b), we provide a systematic survey of
code-switching research in NLP, tracing its evolution from linguistic
theories to modern machine learning. We analyze decades of progress to
highlight key trends, challenges, and future directions.</p>

<p>Shortly after the release of ChatGPT, we noted in Yong et al. (2023)
that it struggled to generate and understand CS/CM, although current
models have improved significantly. This work was highlighted in a
<em>Wired</em> article regarding the multilingual limitations of early ChatGPT.
In Cahyawijaya et al. (2025b), we find that multilingual LLMs
consistently fail to distinguish the meanings of false friends across
languages, revealing major gaps in cross-lingual sense understanding.</p>

<h4 id="multilingual-llms">Multilingual LLMs</h4>

<p>In parallel with dataset and benchmark building, I collaborate on
building multilingual models, including
<a href="https://huggingface.co/bigscience/mt0-base">mT0</a> and
<a href="https://huggingface.co/bigscience/bloomz">BLOOMZ</a> (Muennighoff et al.,
2023), the Arabic‑centric
<a href="https://huggingface.co/inceptionai/jais-13b">Jais</a> (Sengupta et al.,
2023), and the Indonesian LLM
<a href="https://huggingface.co/indonlp/cendol">Cendol</a> (Cahyawijaya et al.,
2024). Particularly, BLOOMZ attracted significant traction and gained
decent citations and downloads, with more than 1M downloads of all time,
and it is still widely downloaded now. Since joining Google as a
visiting researcher, I have also worked on the multilinguality aspect of
Gemini.</p>

<h4 id="interpretability-and-understanding-of-multilingual-models">Interpretability and Understanding of Multilingual Models</h4>

<p>Recently, I explored the intersection of multilinguality and
interpretability. In (Rahmanisa et al., 2025), we find that amplifying
language-specific neurons in multilingual models boosts performance in
their respective languages, particularly low-resource ones, but often
harms cross-lingual generalization. Separately, in (Andrylie et al.,
2025) we use sparse auto encoders to identify interpretable neurons
associated with particular languages, showing that multilingual models
encode clear language-specific representations within their internal
layers.</p>

<h2 id="lightweight-nlp-systems">Lightweight NLP Systems</h2>

<h4 id="fast-machine-translation-system">Fast Machine Translation System</h4>

<p>During my PhD, I worked on fast machine translation systems. In (Aji and
Heafield, 2020), we explored quantization techniques for neural machine
translation and achieved 4-bit precision using a log-based quantization
approach. Building on that, I collaborated with others in a shared task
on efficient machine translation. By combining quantization, knowledge
distillation, and model pruning, we achieved the best overall
performance (Bogoychev et al., 2020). Although I no longer work
exclusively on machine translation, my current research on lightweight
models continues in the same direction.</p>

<h4 id="lightweight-models-via-distillation">Lightweight Models via Distillation</h4>

<p>During the early days of GPT, we distilled ChatGPT into several
smaller-sized models smaller than 1B parameters with, back then,
reasonable performance in our Lamini-LM project (Wu et al., 2024).
Lamini models are still one of the most downloaded models in MBZUAI’s
HuggingFace repo and gained more than 800 GitHub stars.</p>

<p>Some of the lightweight model efforts focus on multilingual
capabilities. For example, Bactrian-X is a distilled multilingual model
that covers 52 languages (Li et al., 2023). We have also attempted to
distill a large multilingual encoder model for low-resource
languages (Cruz, 2025).</p>

<h4 id="sink-free-attention-transformers">Sink-Free Attention Transformers</h4>

<p>In our ongoing work (Zuhri et al., 2025b), we proposed a softmax
replacement named SoftPick, whose aim is to remove the attention sink.
We managed to remove the attention sink, thus making the attention
sparse.</p>

<h4 id="knowledge-distillation-study">Knowledge Distillation Study</h4>

<p>In (Aji et al., 2020), we show that in neural machine translation
transfer learning, copying the inner layers of a model is essential for
quality gains. Our recent work in (Wibowo et al., 2025) similarly
investigates model copying in knowledge distillation across multilingual
settings.</p>

<p>We also study the potential harm of knowledge distillation. In (Mansurov
et al., 2025), we find that leaked data (such as test data) can also be
accidentally leaked by knowledge distillation. At the moment, we are
investigating leakage of PIIs or poisoned data via distillation.</p>

<h2 id="efficient-training-of-nlp-systems">Efficient Training of NLP Systems</h2>

<p>Not only inference, but lack of data could also be the issue of
inaccessible NLP systems. Some of my work explores faster or better
learning.</p>

<h4 id="effective-language-extension-of-nlp-models">Effective Language Extension of NLP Models</h4>

<p>With the lack of training data for many languages, we investigate
various methods for tackling this. In (Adilazuarda et al., 2024a), we
enable unseen generalization of encoder models through an additional
loss, in which we ask the model to learn language representation vector
of the input and use URIEL vectors as the label. This method
significantly boosts the performance of some unseen languages, such as
Amharic.</p>

<p>In (Elshabrawy et al., 2025), we enable zero-shot language and task
generalization for encoder models by training the model with true/false
statements across languages, enabling ‘prompting’ for encoder models.
This project was part of MBZUAI’s 2024 internship project with talented
interns visiting for a month. Our project was awarded the best project.</p>

<h4 id="multi-token-learning">Multi-Token Learning</h4>

<p>In ongoing work in (Zuhri et al., 2025a), we proposed a new learning
objective to learn from multiple tokens at once, with the aim of better
training the model. Specifically, we instruct the model to predict token
ordering.</p>

<h2 id="multimodal-multicultural-nlp">Multimodal-Multicultural NLP</h2>

<p>Most of my work has focused purely on text. With the advancement of AI
technology going beyond text, multimodality is the next direction that
fits my overarching goal that I recently explored.</p>

<h4 id="multimodal-multicultural-datasets-and-benchmarks">Multimodal-Multicultural Datasets and Benchmarks</h4>

<p>I have been working on data set construction for a while; hence,
multimodal datasets were one of the extensions. CVQA is one of the
largest human-made multimodal multilingual datasets that we created. I
lead this initiative with the support of communities to construct
culturally relevant visual question answering for more than 30 language
and country pairs.</p>

<p>In (Winata et al., 2025), we gather images of food and cuisine from
around the world and annotate them. A recent follow-up work on that in
(Irawan et al., 2025), in which we perform adversarial image editing by
replacing the background with landmarks of different countries, or by
adding flags of different countries, noted that VLMs are easily
distracted.</p>

<h4 id="multimodal-multicultural-models">Multimodal-Multicultural Models</h4>

<p>In an ongoing project with the SEACrowd community, we are building
<a href="https://seacrowd.org/projects/2025-seavl-phase-2">SeaVL</a>, a multimodal
language model for Southeast Asian demographics. We have started with
the dataset (Cahyawijaya et al., 2025a) and are now working on the
model. Another ongoing project focuses on building multilingual,
multimodal reward models.</p>

<p>####</p>

<p>Beyond what is mentioned above, I see interesting directions that fit,
such as efficient multimodal architectures.</p>

<h2 id="human-computer-interaction-of-nlp-systems">Human-Computer Interaction of NLP Systems</h2>

<p>I have recently gained a strong interest in the area of Human-AI
interaction, especially with the goal of developing inclusive and
accessible NLP technology. This is particularly important because
different demographics, including cultural backgrounds, can influence
how people perceive and expect AI to behave.</p>

<h4 id="human-bias-in-judging-llm-output">Human Bias in Judging LLM Output</h4>

<p>In Wu and Aji (2025), we explored typical human-preference evaluations
commonly used in standard leaderboards and noted that humans exhibit a
bias towards output length and grammatical correctness, to such a degree
that they prefer hallucinated outputs provided they are long and
grammatically polished.</p>

<p>In our follow-up work (Chevi et al., 2025), we found that this
preference correlates with the user’s personality traits. Specifically,
we observed that users with different personality profiles prioritize
distinct aspects of model responses, suggesting that a single universal
reward model is insufficient to capture the diversity of human
preferences.</p>

<p> <br />
 </p>

<p>My experience in this area is not extensive, and I am still learning and
collaborating. However, I see many interesting directions that intersect
with my research interests. In ongoing work, we are exploring Indonesian
teachers’ AI literacy and its impact on teaching pedagogy. We hope to
uncover the current level of AI literacy in Indonesia and provide
recommendations to policymakers to ensure that AI is used appropriately
in classroom activities.</p>

<div id="refs" class="references csl-bib-body">

<div id="ref-adilazuarda-etal-2025-surveys" class="csl-entry">

Farid Adilazuarda, Chen Cecilia Liu, Iryna Gurevych, and Alham Fikri
Aji. 2025a. \[From surveys to narratives: Rethinking cultural value
adaptation in LLMs\](&lt;https://aclanthology.org/2025.emnlp-main.912/&gt;).
In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and
Violet Peng, editors, *Proceedings of the 2025 conference on empirical
methods in natural language processing*, pages 18063–18090, Suzhou,
China. Association for Computational Linguistics.

</div>


Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Ayu
Purwarianti, and Alham Fikri Aji. 2024a. [LinguAlchemy: Fusing
typological and geographical elements for unseen language
generalization](https://aclanthology.org/2024.findings-emnlp.225). In
*Findings of the association for computational linguistics: EMNLP 2024*,
pages 3912–3928, Miami, Florida, USA. Association for Computational
Linguistics.


<div id="ref-adilazuarda-etal-2024-towards" class="csl-entry">

Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania,
Siddhant Shivdutt Singh, Alham Fikri Aji, Jacki O’Neill, Ashutosh Modi,
and Monojit Choudhury. 2024b. \[Towards measuring and modeling “culture”
in LLMs: A survey\](&lt;https://aclanthology.org/2024.emnlp-main.882&gt;). In
*Proceedings of the 2024 conference on empirical methods in natural
language processing*, pages 15763–15784, Miami, Florida, USA.
Association for Computational Linguistics.

</div>

<div id="ref-adilazuarda-etal-2025-nusaaksara" class="csl-entry">

Muhammad Farid Adilazuarda, Musa Izzanardi Wijanarko, Lucky Susanto,
Khumaisa Nur’aini, Derry Tanti Wijaya, and Alham Fikri Aji. 2025b.
\[NusaAksara: A multimodal and multilingual benchmark for preserving
Indonesian indigenous
scripts\](&lt;https://aclanthology.org/2025.acl-long.1377&gt;). In
*Proceedings of the 63rd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 28371–28401,
Vienna, Austria. Association for Computational Linguistics.

</div>

<div id="ref-aji-etal-2020-neural" class="csl-entry">

Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, and Rico Sennrich.
2020. \[In neural machine translation, what does transfer learning
transfer?\](&lt;https://aclanthology.org/2020.acl-main.688&gt;). In
*Proceedings of the 58th annual meeting of the association for
computational linguistics*, pages 7701–7710, Online. Association for
Computational Linguistics.

</div>

<div id="ref-aji-heafield-2020-compressing" class="csl-entry">

Alham Fikri Aji and Kenneth Heafield. 2020. \[Compressing neural machine
translation models with 4-bit
precision\](&lt;https://aclanthology.org/2020.ngt-1.4&gt;). In *Proceedings of
the fourth workshop on neural generation and translation*, pages 35–42,
Online. Association for Computational Linguistics.

</div>

<div id="ref-aji-etal-2022-one" class="csl-entry">

Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade
Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko
Prasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022. \[One
country, 700+ languages: NLP challenges for underrepresented languages
and dialects in
Indonesia\](&lt;https://aclanthology.org/2022.acl-long.500&gt;). In
*Proceedings of the 60th annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 7226–7249,
Dublin, Ireland. Association for Computational Linguistics.

</div>

<div id="ref-andrylie2025sparse" class="csl-entry">

Lyzander Marciano Andrylie, Inaya Rahmanisa, Mahardika Krisna Ihsani,
Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, and Alham Fikri Aji.
2025. \[Sparse autoencoders can capture language-specific concepts
across diverse languages\](&lt;https://arxiv.org/abs/2507.11230&gt;).

</div>

<div id="ref-bogoychev-etal-2020-edinburghs" class="csl-entry">

Nikolay Bogoychev, Roman Grundkiewicz, Alham Fikri Aji, Maximiliana
Behnke, Kenneth Heafield, Sidharth Kashyap, Emmanouil-Ioannis
Farsarakis, and Mateusz Chudyk. 2020. \[Edinburgh’s submissions to the
2020 machine translation efficiency
task\](&lt;https://aclanthology.org/2020.ngt-1.26/&gt;). In Alexandra Birch,
Andrew Finch, Hiroaki Hayashi, Kenneth Heafield, Marcin Junczys-Dowmunt,
Ioannis Konstas, Xian Li, Graham Neubig, and Yusuke Oda, editors,
*Proceedings of the fourth workshop on neural generation and
translation*, pages 218–224, Online. Association for Computational
Linguistics.

</div>

<div id="ref-cahyawijaya-etal-2023-nusacrowd" class="csl-entry">

<span class="nocase">Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji,
Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono,
Ade Romadhony, Karissa Vincentio, Fajri Koto, Jennifer Santoso, David
Moeljadi, Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmonangan, Ika
Alfina, Muhammad Satrio Wicaksono, Ilham Firdausi Putra, et al.</span>
2023a. \[NusaCrowd: Open source initiative for Indonesian NLP
resources\](&lt;https://aclanthology.org/2023.findings-acl.868&gt;). In
*Findings of the association for computational linguistics: ACL 2023*,
pages 13745–13818, Toronto, Canada. Association for Computational
Linguistics.

</div>

<div id="ref-cahyawijaya-etal-2023-nusawrites" class="csl-entry">

Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista, Emmanuel
Dave, Sarah Oktavianti, Salsabil Akbar, Jhonson Lee, Nuur Shadieq, Tjeng
Wawan Cenggoro, Hanung Wahyuning Linuwih, Bryan Wilie, Galih Pradipta
Muridan, Genta Indra Winata, David Moeljadi, Alham Fikri Aji, Ayu
Purwarianti, and Pascale Fung. 2023b. \[NusaWrites: Constructing
high-quality corpora for underrepresented and extremely low-resource
languages\](&lt;https://aclanthology.org/2023.ijcnlp-main.60&gt;). In
*Proceedings of the 13th international joint conference on natural
language processing and the 3rd conference of the asia-pacific chapter
of the association for computational linguistics (volume 1: Long
papers)*, pages 921–945, Nusa Dua, Bali. Association for Computational
Linguistics.

</div>

<div id="ref-cahyawijaya-etal-2024-cendol" class="csl-entry">

Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri,
Emmanuel Dave, Jhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil
Maulana Akbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan
Wilie, Genta Indra Winata, Alham Fikri Aji, Ayu Purwarianti, and Pascale
Fung. 2024. \[Cendol: Open instruction-tuned generative large language
models for Indonesian
languages\](&lt;https://aclanthology.org/2024.acl-long.796&gt;). In
*Proceedings of the 62nd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 14899–14914,
Bangkok, Thailand. Association for Computational Linguistics.

</div>

<div id="ref-cahyawijaya-etal-2025-crowdsource" class="csl-entry">

Samuel Cahyawijaya, Holy Lovenia, Joel Ruben Antony Moniz, Tack Hwa
Wong, Mohammad Rifqi Farhansyah, Thant Thiri Maung, Frederikus Hudi,
David Anugraha, Muhammad Ravi Shulthan Habibi, Muhammad Reza Qorib, Amit
Agarwal, Joseph Marvin Imperial, Hitesh Laxmichand Patel, Vicky Feliren,
Bahrul Ilmi Nasution, Manuel Antonio Rufino, Genta Indra Winata, Rian
Adam Rajagede, Carlos Rafael Catalan, et al. 2025a. \[Crowdsource,
crawl, or generate? Creating SEA-VL, a multicultural vision-language
dataset for Southeast
Asia\](&lt;https://aclanthology.org/2025.acl-long.916/&gt;). In Wanxiang Che,
Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors,
*Proceedings of the 63rd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 18685–18717,
Vienna, Austria. Association for Computational Linguistics.

</div>

<div id="ref-cahyawijaya-etal-2025-stingray" class="csl-entry">

Samuel Cahyawijaya, Ruochen Zhang, Jan Christian Blaise Cruz, Holy
Lovenia, Elisa Gilbert, Hiroki Nomoto, and Alham Fikri Aji. 2025b.
\[Thank you, stingray: Multilingual large language models can not (yet)
disambiguate cross-lingual word
senses\](&lt;https://aclanthology.org/2025.findings-naacl.178&gt;). In
*Findings of the association for computational linguistics: NAACL 2025*,
pages 3228–3250, Albuquerque, New Mexico. Association for Computational
Linguistics.

</div>

<div id="ref-chevi-2025-individual" class="csl-entry">

Rendi Chevi, Kentaro Inui, Thamar Solorio, and Alham Fikri Aji. 2025.
\[How individual traits and language styles shape preferences in
open-ended user-LLM interaction: A preliminary
study\](&lt;https://arxiv.org/abs/2504.17083&gt;).

</div>

<div id="ref-cruz-2025-extracting" class="csl-entry">

Jan Christian Blaise Cruz. 2025. \[Extracting general-use transformers
for low-resource languages via knowledge
distillation\](&lt;https://aclanthology.org/2025.loreslm-1.17/&gt;). In Hansi
Hettiarachchi, Tharindu Ranasinghe, Paul Rayson, Ruslan Mitkov, Mohamed
Gaber, Damith Premasiri, Fiona Anting Tan, and Lasitha Uyangodage,
editors, *Proceedings of the first workshop on language models for
low-resource languages*, pages 219–224, Abu Dhabi, United Arab Emirates.
Association for Computational Linguistics.

</div>

<div id="ref-elshabrawy-etal-2025-statement" class="csl-entry">

Ahmed Elshabrawy, Thanh-Nhi Nguyen, Yeeun Kang, Lihan Feng, Annant Jain,
Faadil Abdullah Shaikh, Jonibek Mansurov, Mohamed Fazli Mohamed Imam,
Jesus-German Ortiz-Barajas, Rendi Chevi, and Alham Fikri Aji. 2025.
\[Statement-tuning enables efficient cross-lingual generalization in
encoder-only models\](&lt;https://aclanthology.org/2025.findings-acl.835&gt;).
In *Findings of the association for computational linguistics: ACL
2025*, pages 16226–16248, Vienna, Austria. Association for Computational
Linguistics.

</div>

<div id="ref-farhansyah-etal-2025-language" class="csl-entry">

Mohammad Rifqi Farhansyah, Iwan Darmawan, Adryan Kusumawardhana, Genta
Indra Winata, Alham Fikri Aji, and Derry Tanti Wijaya. 2025. \[Do
language models understand honorific systems in
Javanese?\](&lt;https://aclanthology.org/2025.acl-long.1296/&gt;). In Wanxiang
Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar,
editors, *Proceedings of the 63rd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 26732–26754,
Vienna, Austria. Association for Computational Linguistics.

</div>

<div id="ref-irawan2025visionlanguagemodelsconfused" class="csl-entry">

Patrick Amadeus Irawan, Ikhlasul Akmal Hanif, Muhammad Dehan Al Kautsar,
Genta Indra Winata, Fajri Koto, and Alham Fikri Aji. 2025. \[Vision
language models are confused
tourists\](&lt;https://arxiv.org/abs/2511.17004&gt;).

</div>

<div id="ref-li2023bactrian" class="csl-entry">

Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin.
2023. Bactrian-x: Multilingual replicable instruction-following models
with low-rank adaptation. *arXiv preprint arXiv:2305.15011*.

</div>

<div id="ref-lovenia-etal-2024-seacrowd" class="csl-entry">

<span class="nocase">Holy Lovenia, Rahmad Mahendra, Samuel Cahyawijaya,
Genta Indra Winata, Alham Fikri Aji, et al.</span> 2024. \[SEACrowd: A
multilingual multimodal data hub and benchmark suite for southeast Asian
languages\](&lt;https://aclanthology.org/2024.emnlp-main.296&gt;). In
*Proceedings of the 2024 conference on empirical methods in natural
language processing*, pages 13745–13818, Miami, Florida, USA.
Association for Computational Linguistics.

</div>

<div id="ref-mahendra-etal-2021-indonli" class="csl-entry">

Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan, Fahrurrozi Rahman, and
Clara Vania. 2021. \[IndoNLI: A natural language inference dataset for
Indonesian\](&lt;https://aclanthology.org/2021.emnlp-main.821&gt;). In
*Proceedings of the 2021 conference on empirical methods in natural
language processing*, pages 10511–10527, Online; Punta Cana, Dominican
Republic. Association for Computational Linguistics.

</div>

<div id="ref-mansurov-etal-2025-data" class="csl-entry">

Jonibek Mansurov, Akhmed Sakip, and Alham Fikri Aji. 2025. \[Data
laundering: Artificially boosting benchmark results through knowledge
distillation\](&lt;https://aclanthology.org/2025.acl-long.407&gt;). In
*Proceedings of the 63rd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 8332–8345,
Vienna, Austria. Association for Computational Linguistics.

</div>

<div id="ref-muennighoff-etal-2023-crosslingual" class="csl-entry">

Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong,
Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid
Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff,
and Colin Raffel. 2023. \[Crosslingual generalization through multitask
finetuning\](&lt;https://aclanthology.org/2023.acl-long.891&gt;). In
*Proceedings of the 61st annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 15991–16011,
Toronto, Canada. Association for Computational Linguistics.

</div>

<div id="ref-muhammad-etal-2025-brighter" class="csl-entry">

Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan
Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal
Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani,
Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir
Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, et
al. 2025a. \[BRIGHTER: BRIdging the gap in human-annotated textual
emotion recognition datasets for 28
languages\](&lt;https://aclanthology.org/2025.acl-long.436/&gt;). In Wanxiang
Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar,
editors, *Proceedings of the 63rd annual meeting of the association for
computational linguistics (volume 1: Long papers)*, pages 8895–8916,
Vienna, Austria. Association for Computational Linguistics.

</div>

<div id="ref-muhammad-etal-2025-semeval" class="csl-entry">

Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Seid
Muhie Yimam, Jan Philip Wahle, Terry Lima Ruas, Meriem Beloucif,
Christine De Kock, Tadesse Destaw Belay, Ibrahim Said Ahmad, Nirmal
Surange, Daniela Teodorescu, David Ifeoluwa Adelani, Alham Fikri Aji,
Felermino Dario Mario Ali, Vladimir Araujo, Abinew Ali Ayele, Oana
Ignat, Alexander Panchenko, et al. 2025b. \[SemEval-2025 task 11:
Bridging the gap in text-based emotion
detection\](&lt;https://aclanthology.org/2025.semeval-1.327/&gt;). In Sara
Rosenthal, Aiala Rosá, Debanjan Ghosh, and Marcos Zampieri, editors,
*Proceedings of the 19th international workshop on semantic evaluation
(SemEval-2025)*, pages 2558–2569, Vienna, Austria. Association for
Computational Linguistics.

</div>

<div id="ref-rahmanisa2025unveiling" class="csl-entry">

Inaya Rahmanisa, Lyzander Marciano Andrylie, Mahardika Krisna Ihsani,
Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, and Alham Fikri Aji.
2025. \[Unveiling the influence of amplifying language-specific
neurons\](&lt;https://arxiv.org/abs/2507.22581&gt;).

</div>

<div id="ref-romero-etal-2024-cvqa" class="csl-entry">

David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, ..., Thamar
Solorio, and Alham Fikri Aji. 2024. \[CVQA: Culturally-diverse
multilingual visual question answering
benchmark\](&lt;https://proceedings.neurips.cc/paper_files/paper/2024/hash/1568882ba1a50316e87852542523739c-Abstract-Datasets_and_Benchmarks_Track.html&gt;).
In *Advances in neural information processing systems 37 (NeurIPS
2024)*.

</div>

<div id="ref-sen-etal-2022-mintaka" class="csl-entry">

Priyanka Sen, Alham Fikri Aji, and Amir Saffari. 2022. \[Mintaka: A
complex, natural, and multilingual dataset for end-to-end question
answering\](&lt;https://aclanthology.org/2022.coling-1.138&gt;). In
*Proceedings of the 29th international conference on computational
linguistics*, pages 1604–1619, Gyeongju, Republic of Korea.
International Committee on Computational Linguistics.

</div>

<div id="ref-sengupta2023jais" class="csl-entry">

Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan
Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul
Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji,
Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson,
et al. 2023. \[Jais and Jais-chat: Arabic-centric foundation and
instruction-tuned open generative large language
models\](&lt;https://arxiv.org/abs/2308.16149&gt;).

</div>

<div id="ref-wang-etal-2024-semeval" class="csl-entry">

Yuxia Wang, Alham Fikri Aji, Artem Shelmanov, Chenxi Whitehouse, Petar
Ivanov, Jonibek Mansurov, Jinyan Su, Tarek Mahmoud, Osama Mohammed
Afzal, Akim Tsvigun, Toru Sasaki, Thomas Arnold, Nizar Habash, Iryna
Gurevych, and Preslav Nakov. 2024a. \[SemEval-2024 task 8: Multidomain,
multimodel and multilingual machine-generated text
detection\](&lt;https://aclanthology.org/2024.semeval-1.148&gt;). In
*Proceedings of the 18th international workshop on semantic evaluation
(SemEval-2024)*, pages 1133–1159, Mexico City, Mexico. Association for
Computational Linguistics.

</div>

<div id="ref-wang-etal-2024-m4" class="csl-entry">

Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov,
Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud,
Toru Sasaki, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna
Gurevych, and Preslav Nakov. 2024b. \[M4: Multi-generator, multi-domain,
and multi-lingual black-box machine-generated text
detection\](&lt;https://aclanthology.org/2024.eacl-long.83/&gt;). In Yvette
Graham and Matthew Purver, editors, *Proceedings of the 18th conference
of the european chapter of the association for computational linguistics
(volume 1: Long papers)*, pages 1369–1407, St. Julian’s, Malta.
Association for Computational Linguistics.

</div>

<div id="ref-wibowo2025iterabre" class="csl-entry">

Haryo Akbarianto Wibowo, Haiyue Song, Hideki Tanaka, Masao Utiyama,
Alham Fikri Aji, and Raj Dabre. 2025. \[IteRABRe: Iterative
recovery-aided block reduction\](&lt;https://arxiv.org/abs/2503.06291&gt;).

</div>

<div id="ref-wibowo-etal-2024-copal" class="csl-entry">

Haryo Wibowo, Erland Fuadi, Made Nityasya, Radityo Eko Prasojo, and
Alham Aji. 2024. \[COPAL-ID: Indonesian language reasoning with local
culture and nuances\](&lt;https://aclanthology.org/2024.naacl-long.77&gt;). In
*Proceedings of the 2024 conference of the north american chapter of the
association for computational linguistics: Human language technologies
(volume 1: Long papers)*, pages 1404–1422, Mexico City, Mexico.
Association for Computational Linguistics.

</div>

<div id="ref-winata-etal-2023-nusax" class="csl-entry">

Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad
Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi,
Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico
Sennrich, and Sebastian Ruder. 2023a. \[NusaX: Multilingual parallel
sentiment dataset for 10 Indonesian local
languages\](&lt;https://aclanthology.org/2023.eacl-main.57&gt;). In
*Proceedings of the 17th conference of the european chapter of the
association for computational linguistics*, pages 815–834, Dubrovnik,
Croatia. Association for Computational Linguistics.

</div>

<div id="ref-winata-etal-2025-worldcuisines" class="csl-entry">

Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David
Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq
Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das,
Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching
Lam Cheng, Daud Abolade, Emmanuele Chersoni, et al. 2025.
\[WorldCuisines: A massive-scale benchmark for multilingual and
multicultural visual question answering on global
cuisines\](&lt;https://aclanthology.org/2025.naacl-long.167&gt;). In
*Proceedings of the 2025 conference of the nations of the americas
chapter of the association for computational linguistics: Human language
technologies (volume 1: Long papers)*, pages 3242–3264, Albuquerque, New
Mexico. Association for Computational Linguistics.

</div>

<div id="ref-winata-etal-2023-decades" class="csl-entry">

Genta Winata, Alham Fikri Aji, Zheng Xin Yong, and Thamar Solorio.
2023b. \[The decades progress on code-switching research in NLP: A
systematic survey on trends and
challenges\](&lt;https://aclanthology.org/2023.findings-acl.185&gt;). In
*Findings of the association for computational linguistics: ACL 2023*,
pages 2936–2978, Toronto, Canada. Association for Computational
Linguistics.

</div>

<div id="ref-wu-aji-2025-style" class="csl-entry">

Minghao Wu and Alham Fikri Aji. 2025. \[Style over substance: Evaluation
biases for large language
models\](&lt;https://aclanthology.org/2025.coling-main.21/&gt;). In Owen
Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di
Eugenio, and Steven Schockaert, editors, *Proceedings of the 31st
international conference on computational linguistics*, pages 297–312,
Abu Dhabi, UAE. Association for Computational Linguistics.

</div>

<div id="ref-wu-etal-2024-lamini" class="csl-entry">

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham
Fikri Aji. 2024. \[LaMini-LM: A diverse herd of distilled models from
large-scale
instructions\](&lt;https://aclanthology.org/2024.eacl-long.57&gt;). In
*Proceedings of the 18th conference of the european chapter of the
association for computational linguistics (volume 1: Long papers)*,
pages 944–964, St. Julian’s, Malta. Association for Computational
Linguistics.

</div>

<div id="ref-yong-etal-2023-prompting" class="csl-entry">

Zheng Xin Yong, Ruochen Zhang, Jessica Forde, Skyler Wang, Arjun
Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Winata, Lintang
Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Long Phan,
Rowena Garcia, Thamar Solorio, and Alham Fikri Aji. 2023. \[Prompting
multilingual large language models to generate code-mixed texts: The
case of south East Asian
languages\](&lt;https://aclanthology.org/2023.calcs-1.5/&gt;). In Genta
Winata, Sudipta Kar, Marina Zhukova, Thamar Solorio, Mona Diab, Sunayana
Sitaram, Monojit Choudhury, and Kalika Bali, editors, *Proceedings of
the 6th workshop on computational approaches to linguistic
code-switching*, pages 43–63, Singapore. Association for Computational
Linguistics.

</div>

<div id="ref-zuhri2025predicting" class="csl-entry">

Zayd M. K. Zuhri, Erland Hilman Fuadi, and Alham Fikri Aji. 2025a.
\[Predicting the order of upcoming tokens improves language
modeling\](&lt;https://arxiv.org/abs/2508.19228&gt;).

</div>

<div id="ref-zuhri2025softpick" class="csl-entry">

Zayd M. K. Zuhri, Erland Hilman Fuadi, and Alham Fikri Aji. 2025b.
\[Softpick: No attention sink, no massive activations with rectified
softmax\](&lt;https://arxiv.org/abs/2504.20966&gt;).

</div>

</div>

<style>
  /* Container for the bibliography */
  div.csl-bib-body {
    line-height: 1.5;
    margin-top: 2rem;
  }
  /* Individual entries: Hanging indent */
  div.csl-entry {
    margin-bottom: 1rem;
    text-indent: -1.5em;
    padding-left: 1.5em;
    clear: both;
  }
</style>


        </section>
      </article>
    </div>

    <div class="page__footer">
      <footer>
      </footer>
    </div>

    <script src="https://afaji.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

